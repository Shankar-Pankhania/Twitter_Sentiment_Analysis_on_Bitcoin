{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_LSTM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMLDuORq6hyp5DHBgqnrZM1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shankar-Pankhania/Twitter_Sentiment_Analysis_on_Bitcoin/blob/main/Train_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8smQkhL69al",
        "outputId": "c15156f9-b5aa-47be-eda8-8b8f26f7d3c2"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/My Drive/Dissertation/"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/My Drive/Dissertation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A34lDvJLSKe4"
      },
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXgSQounRFAc",
        "outputId": "a31856d7-4c9b-4291-a46e-f061d154b37d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, SpatialDropout1D, Conv1D, MaxPooling1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "tf.config.list_physical_devices('GPU')   #check GPU connected"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sdm5KbVPWlHo"
      },
      "source": [
        "Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFvzgJ6S7ZVP"
      },
      "source": [
        "total_dataset = pd.read_csv('bitcoin_tweets.csv')\n",
        "total_dataset = total_dataset[total_dataset.sentiment != 0] #remove neutral tweets, only need positve and negative sentiments.\n",
        "total_dataset['sentiment'] = total_dataset['sentiment'].replace(-1, 0) #replaces negative sentiment -1 with 0 for analysis"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdntfzuKobYp"
      },
      "source": [
        "Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaMBOLPZoavl"
      },
      "source": [
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures)#(ngram_range=(1,2)\n",
        "tokenizer.fit_on_texts(total_dataset['processed_tweet'].values)\n",
        "X = tokenizer.texts_to_sequences(total_dataset['processed_tweet'].values)\n",
        "X = pad_sequences(X)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIFMIjb_8aPW"
      },
      "source": [
        "LSTM Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey5LH5d28cda",
        "outputId": "5f3e35ec-edd0-45ba-8194-8031f7648806"
      },
      "source": [
        "embed_dim = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(2000, embed_dim,input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(Conv1D(64, 5, activation='tanh'))\n",
        "model.add(MaxPooling1D(pool_size=4))\n",
        "model.add(CuDNNLSTM(128))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 116, 128)          256000    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 116, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 112, 64)           41024     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 28, 64)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     (None, 128)               99328     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 396,610\n",
            "Trainable params: 396,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wCNpFooiBVV"
      },
      "source": [
        "Splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVWe_5yELg5J",
        "outputId": "0ff18c10-506e-43ac-8817-54da51b38257"
      },
      "source": [
        "Y = pd.get_dummies(total_dataset['sentiment']).values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 69)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(57056, 116) (57056, 2)\n",
            "(14264, 116) (14264, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaHDUMxaCBXM"
      },
      "source": [
        "K Fold Cross Validation for Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfxoIPZ0CEo0"
      },
      "source": [
        "#The code below has been commented out because we will be using test/train split for this sentiment analysis. If you would like to check, please uncomment the code and comment the test/train split code and run the whole code again.\n",
        "\n",
        "\"\"\"\n",
        "num_folds = 10 \n",
        "# Parse numbers as floats\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# Normalize data\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "# Merge inputs and targets\n",
        "inputs = np.concatenate((X_train, X_test), axis=0)\n",
        "targets = np.concatenate((Y_train, Y_test), axis=0)\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "fold_no = 1\n",
        "batch_size = 32\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "  \n",
        "  #fit data to model\n",
        "  model.fit(inputs[train], targets[train], epochs = 4, batch_size=batch_size, verbose = 2)\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9FTqlEcU0q3"
      },
      "source": [
        "Train the Network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptia5UZoKShn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed45723-d787-49c4-fbf6-f027d06463eb"
      },
      "source": [
        "model.fit(X_train, Y_train, epochs = 10)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1783/1783 [==============================] - 17s 9ms/step - loss: 0.3402 - accuracy: 0.8466\n",
            "Epoch 2/10\n",
            "1783/1783 [==============================] - 16s 9ms/step - loss: 0.1681 - accuracy: 0.9334\n",
            "Epoch 3/10\n",
            "1783/1783 [==============================] - 16s 9ms/step - loss: 0.1457 - accuracy: 0.9414\n",
            "Epoch 4/10\n",
            "1783/1783 [==============================] - 16s 9ms/step - loss: 0.1253 - accuracy: 0.9504\n",
            "Epoch 5/10\n",
            "1783/1783 [==============================] - 16s 9ms/step - loss: 0.1068 - accuracy: 0.9572\n",
            "Epoch 6/10\n",
            "1783/1783 [==============================] - 16s 9ms/step - loss: 0.0883 - accuracy: 0.9642\n",
            "Epoch 7/10\n",
            "1783/1783 [==============================] - 16s 9ms/step - loss: 0.0761 - accuracy: 0.9696\n",
            "Epoch 8/10\n",
            "1783/1783 [==============================] - 16s 9ms/step - loss: 0.0700 - accuracy: 0.9725\n",
            "Epoch 9/10\n",
            "1783/1783 [==============================] - 17s 9ms/step - loss: 0.0605 - accuracy: 0.9756\n",
            "Epoch 10/10\n",
            "1783/1783 [==============================] - 16s 9ms/step - loss: 0.0566 - accuracy: 0.9783\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f164995e210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO-hZPeNXjUr"
      },
      "source": [
        "Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu9-uf4-XkZm",
        "outputId": "0fdc1f99-1b1d-4831-81c2-e739506c8a23"
      },
      "source": [
        "validation_size = 2000\n",
        "\n",
        "X_validate = X_test[-validation_size:]\n",
        "Y_validate = Y_test[-validation_size:]\n",
        "X_test = X_test[:-validation_size]\n",
        "Y_test = Y_test[:-validation_size]\n",
        "score,acc = model.evaluate(X_test, Y_test)\n",
        "print(\"Accuracy with LSTM:\" + str(acc))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "384/384 [==============================] - 2s 4ms/step - loss: 0.2829 - accuracy: 0.9247\n",
            "Accuracy with LSTM:0.9246575236320496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJLLVHjBz6-y"
      },
      "source": [
        "Measure number of correct guesses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbZt847akzqF"
      },
      "source": [
        "#initialise values\n",
        "positive_count, negative_count, positive_correct, negative_correct = 0, 0, 0, 0\n",
        "for x in range(len(X_validate)):\n",
        "    \n",
        "    prediction = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
        "   \n",
        "    if np.argmax(prediction) == np.argmax(Y_validate[x]):\n",
        "        if np.argmax(Y_validate[x]) == 0:\n",
        "            negative_correct += 1\n",
        "        else:\n",
        "            positive_correct += 1\n",
        "       \n",
        "    if np.argmax(Y_validate[x]) == 0:\n",
        "        negative_count += 1\n",
        "    else:\n",
        "        positive_count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7QCfnZflZ5X",
        "outputId": "e59e01b8-4423-4f53-bbb6-a59ed5d04c05"
      },
      "source": [
        "print(\"positive_accuracy\", positive_correct/positive_count*100, \"%\")\n",
        "print(\"negative_accuracy\", negative_correct/negative_count*100, \"%\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive_accuracy 95.2127659574468 %\n",
            "negative_accuracy 86.29032258064517 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
